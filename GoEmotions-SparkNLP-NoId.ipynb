{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccdb2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip --quiet install spark-nlp==3.4.0 pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e6ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sparknlp\n",
    "import pyspark\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.json4s:json4s-native_3:4.0.3,com.amazonaws:aws-java-sdk:1.12.136,org.apache.hadoop:hadoop-aws:3.3.1,com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.0 pyspark-shell'\n",
    "\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id,split, explode, concat_ws, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26c86d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 3.4.0\n",
      "PySpark version: <module 'pyspark.version' from '/home/emr-notebook/.local/lib/python3.7/site-packages/pyspark/version.py'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"PySpark version:\", pyspark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbece8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark configuration\n",
    "conf = SparkConf()\\\n",
    ".set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true') \\\n",
    ".set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true') \\\n",
    ".set(\"spark.driver.memory\", \"4G\") \\\n",
    ".set(\"spark.driver.maxResultSize\", \"0\") \\\n",
    ".set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    ".set(\"spark.kryoserializer.buffer.max\", \"2000m\") \\\n",
    ".set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.0\") \\\n",
    ".setAppName('SPARK NLP AWS').setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c007e28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc=pyspark.SparkContext(conf=conf)\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2959a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessKeyId='your access Key'\n",
    "secretAccessKey='your secret key'\n",
    "\n",
    "train_URI = \"s3a://erasolon-ml-dataset/GoEmotions/train.tsv\"\n",
    "train_output_URI = \"s3a://erasolon-ml-output/emr/goemotions/train_output\"\n",
    "\n",
    "validation_URI = \"s3a://erasolon-ml-dataset/GoEmotions/dev.tsv\"\n",
    "validation_output_URI = \"s3a://erasolon-ml-output/emr/goemotions/validation_output\"\n",
    "\n",
    "test_URI = \"s3a://erasolon-ml-dataset/GoEmotions/test.tsv\"\n",
    "test_output_URI = \"s3a://erasolon-ml-output/emr/goemotions/test_output\"\n",
    "\n",
    "label_URI = \"s3a://erasolon-ml-dataset/GoEmotions/labels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05844e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc98f7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_label_df = spark.read.csv(label_URI,header=False,inferSchema=True)\n",
    "raw_train_df=spark.read.csv(train_URI,header=False,inferSchema=True,sep=\"\\t\")\n",
    "raw_validation_df=spark.read.csv(validation_URI,header=False,inferSchema=True,sep=\"\\t\")\n",
    "raw_test_df=spark.read.csv(test_URI,header=False,inferSchema=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7c93df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "renamed_label_df = raw_label_df.withColumnRenamed('_c0', 'label').withColumn(\"id_label\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa0e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "   .setInputCols([\"document\"]) \\\n",
    "   .setOutputCol(\"sentences\")\n",
    "    \n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentences\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "      \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"normalized\")\\\n",
    "    .setOutputCol(\"cleanTokens\")\\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "tokenAssembler = TokenAssembler() \\\n",
    "   .setInputCols([\"sentences\", \"lemma\"]) \\\n",
    "   .setOutputCol(\"cleanText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f0e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            sentenceDetector, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            lemma,\n",
    "           tokenAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095dc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, pipeline):\n",
    "    splitted_raw_df = df.withColumn('_c1',explode(split('_c1',',')))\n",
    "    renamed_df = splitted_raw_df.select('_c0','_c1').withColumnRenamed('_c0', 'text').withColumnRenamed('_c1', 'id_label')\n",
    "    renamed_df = renamed_df.join(renamed_label_df,\"id_label\",\"inner\").drop(\"id_label\")\n",
    "    renamed_df = renamed_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "    # Annotate your testing dataset\n",
    "    cleaned_df = pipeline.fit(renamed_df).transform(renamed_df).select(\"cleanText.result\")\n",
    "    cleaned_df = cleaned_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "    final_df = cleaned_df.join(renamed_df, \"id\", \"inner\").drop('text','id').withColumn('source',concat_ws(\" \", 'result')).drop(\"result\")\n",
    "    return final_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5950be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = clean_df(raw_train_df, clf_pipeline)\n",
    "final_validation_df = clean_df(raw_validation_df, clf_pipeline)\n",
    "final_test_df = clean_df(raw_test_df, clf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff606d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blazingtext_format(source, label):\n",
    "    return \"__label__\"+label+\" \"+source\n",
    "\n",
    "blazingTextUDF = udf(lambda x, z: blazingtext_format(x,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a625b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_train_df = final_train_df.select(blazingTextUDF(\"source\",\"label\"))\n",
    "manifest_validation_df = final_validation_df.select(blazingTextUDF(\"source\",\"label\"))\n",
    "manifest_test_df = final_test_df.select(blazingTextUDF(\"source\",\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b162ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_train_df.repartition(1).write.text(train_output_URI)\n",
    "manifest_validation_df.repartition(1).write.text(validation_output_URI)\n",
    "manifest_test_df.repartition(1).write.text(test_output_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55ebd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
